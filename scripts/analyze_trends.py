#!/usr/bin/env python3
import json
import os
import glob
from datetime import datetime, timedelta
import statistics

def load_metrics():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ø–∞–ø–∫–∏ logs/daily_metrics"""
    metrics = []
    pattern = "logs/daily_metrics/*.json"
    
    for filename in glob.glob(pattern):
        try:
            with open(filename, 'r') as f:
                data = json.load(f)
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                date_str = os.path.basename(filename).replace('.json', '')
                data['date'] = date_str
                metrics.append(data)
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {filename}: {e}")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
    metrics.sort(key=lambda x: x['date'])
    return metrics

def analyze_trends(metrics):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–¥—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    if len(metrics) < 2:
        print("üìä –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤ (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 –¥–Ω—è)")
        return
    
    print("üìà –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    print("=" * 50)
    
    # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π
    recent = metrics[-7:] if len(metrics) >= 7 else metrics
    
    # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    avg_cv = statistics.mean([m.get('cv_latency', 0) for m in recent])
    avg_p95 = statistics.mean([m.get('p95_latency_ms', 0) for m in recent])
    avg_p99 = statistics.mean([m.get('p99_latency_ms', 0) for m in recent])
    
    print(f"üìä –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞ {len(recent)} –¥–Ω–µ–π:")
    print(f"   CV: {avg_cv:.1f}%")
    print(f"   P95: {avg_p95:.1f}ms")
    print(f"   P99: {avg_p99:.1f}ms")
    
    # –¢—Ä–µ–Ω–¥—ã
    if len(metrics) >= 3:
        first_week = metrics[:3]
        last_week = metrics[-3:]
        
        first_avg_cv = statistics.mean([m.get('cv_latency', 0) for m in first_week])
        last_avg_cv = statistics.mean([m.get('cv_latency', 0) for m in last_week])
        
        cv_trend = "üìà" if last_avg_cv > first_avg_cv else "üìâ"
        print(f"\n{cv_trend} –¢—Ä–µ–Ω–¥ CV: {first_avg_cv:.1f}% ‚Üí {last_avg_cv:.1f}%")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    if avg_cv > 50:
        print("   ‚ö†Ô∏è  –í—ã—Å–æ–∫–∞—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å (CV > 50%)")
        print("   üîç –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ —Å–∏—Å—Ç–µ–º—É")
    elif avg_cv > 30:
        print("   ‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω–∞—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å (CV > 30%)")
        print("   üìä –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥")
    else:
        print("   ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
    
    if avg_p95 > 2000:
        print("   ‚ö†Ô∏è  –í—ã—Å–æ–∫–∏–µ –∑–∞–¥–µ—Ä–∂–∫–∏ (P95 > 2000ms)")
        print("   üîß –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é")
    elif avg_p95 > 1000:
        print("   ‚ö†Ô∏è  –£–º–µ—Ä–µ–Ω–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ (P95 > 1000ms)")
        print("   üìä –¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è")

def main():
    print("üîç –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ Porta")
    print("=" * 30)
    
    metrics = load_metrics()
    
    if not metrics:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        print("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python3 scripts/run_diagnostics.py")
        return
    
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(metrics)} –æ—Ç—á—ë—Ç–æ–≤")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    latest = metrics[-1]
    print(f"\nüìä –ü–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç—á—ë—Ç ({latest['date']}):")
    print(f"   CV: {latest.get('cv_latency', 'N/A')}%")
    print(f"   P95: {latest.get('p95_latency_ms', 'N/A')}ms")
    print(f"   P99: {latest.get('p99_latency_ms', 'N/A')}ms")
    print(f"   –£—Å–ø–µ—à–Ω—ã—Ö: {latest.get('successful', 'N/A')}")
    print(f"   –û—à–∏–±–æ–∫: {latest.get('errors', 'N/A')}")
    
    analyze_trends(metrics)

if __name__ == "__main__":
    main() 